{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, PunktSentenceTokenizer, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('punkt')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#nltk.download('wordnet')\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas, numpy, textblob, string\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert the path of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Insert the path with the clean scores\n",
    "path = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the names and the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = 'D:\\\\filings_clean_withscore'\n",
    "\n",
    "files = []\n",
    "# r=root, d=directories, f=files\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.txt' in file:\n",
    "            files.append(os.path.join(r, file))\n",
    "scores = []\n",
    "names = []\n",
    "for f in files:\n",
    "    scores.append(re.findall(\"_\\d_\\d_\\d_\\d\", f))\n",
    "    names.append(re.findall(\"text_10k_(.*)(?=_\\d_\\d_\\d_)\", f))\n",
    "\n",
    "names_joined = [' '.join(x) for x in names]\n",
    "text_dict  = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert the text in a dictionary and create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, name in zip(files, names_joined):\n",
    "    with open(file, \"r\") as myfile:\n",
    "        text_dict[name] = myfile.read().replace(\"\\n\", \" \")\n",
    "\n",
    "df_text = pd.DataFrame(text_dict.items(), columns=[\"id\", \"text\"])\n",
    "del text_dict, names\n",
    "scores_joined = [' '.join(x) for x in scores]\n",
    "del scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create columns for the ESG scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hella\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_text[\"total_esg\"] = 0\n",
    "df_text[\"E_score\"] = 0\n",
    "df_text[\"S_score\"] = 0\n",
    "df_text[\"G_score\"] = 0\n",
    "\n",
    "for row in range(len(df_text)):\n",
    "    df_text[\"total_esg\"].iloc[row] = scores_joined[row].split(\"_\")[1]\n",
    "    df_text[\"E_score\"].iloc[row] = scores_joined[row].split(\"_\")[2]\n",
    "    df_text[\"S_score\"].iloc[row] = scores_joined[row].split(\"_\")[3]\n",
    "    df_text[\"G_score\"].iloc[row] = scores_joined[row].split(\"_\")[4]\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hella\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "for sen in range(0, len(df_text)):  \n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(df_text[\"text\"][sen]))\n",
    "\n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Replace numbers with \"number\"\n",
    "    document = re.sub(r'[0-9]+', 'number', document)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    document = re.sub(r'[^\\w\\s]', \"\", document)\n",
    "    \n",
    "    # Remove underscore\n",
    "    document = re.sub(r'_+', 'underscore', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "\n",
    "    # Tokenizing\n",
    "    document = document.split()\n",
    "\n",
    "    \n",
    "    # Lemmatizing\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "\n",
    "    documents.append(document)\n",
    "    \n",
    "for row_t in range(len(df_text)):\n",
    "    df_text[\"text\"].iloc[row_t] = documents[row_t]\n",
    "\n",
    "del documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hella\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Hella\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\Hella\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df_text['char_count'] = df_text['text'].apply(len)\n",
    "df_text['word_count'] = df_text['text'].apply(lambda x: len(x.split()))\n",
    "df_text['word_density'] = df_text['char_count'] / (df_text['word_count']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepath locations\n",
    "stopWordsFile = 'StopWords_Generic.txt'\n",
    "positiveWordsFile = 'PositiveWords.txt'\n",
    "nagitiveWordsFile = 'NegativeWords.txt'\n",
    "uncertainty_dictionaryFile = 'uncertainty_dictionary.txt'\n",
    "constraining_dictionaryFile = 'constraining_dictionary.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(stopWordsFile ,'r') as stop_words:\n",
    "    stopWords = stop_words.read().lower()\n",
    "stopWordList = stopWords.split('\\n')\n",
    "stopWordList[-1:] = []\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = text.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    filtered_words = list(filter(lambda token: token not in stopWordList, tokens))\n",
    "    return filtered_words\n",
    "\n",
    "# Loading positive words\n",
    "with open(positiveWordsFile,'r') as posfile:\n",
    "    positivewords=posfile.read().lower()\n",
    "positiveWordList=positivewords.split('\\n')\n",
    "\n",
    "# Loading negative words\n",
    "with open(nagitiveWordsFile ,'r') as negfile:\n",
    "    negativeword=negfile.read().lower()\n",
    "negativeWordList=negativeword.split('\\n')\n",
    "\n",
    "# Calculating positive score \n",
    "def positive_score(text):\n",
    "    numPosWords = 0\n",
    "    rawToken = tokenizer(text)\n",
    "    for word in rawToken:\n",
    "        if word in positiveWordList:\n",
    "            numPosWords  += 1\n",
    "    \n",
    "    sumPos = numPosWords\n",
    "    return sumPos\n",
    "\n",
    "# Calculating Negative score\n",
    "def negative_word(text):\n",
    "    numNegWords=0\n",
    "    rawToken = tokenizer(text)\n",
    "    for word in rawToken:\n",
    "        if word in negativeWordList:\n",
    "            numNegWords -=1\n",
    "    sumNeg = numNegWords \n",
    "    sumNeg = sumNeg * -1\n",
    "    return sumNeg\n",
    "\n",
    "# Calculating polarity score\n",
    "def polarity_score(positiveScore, negativeScore):\n",
    "    pol_score = (positiveScore - negativeScore) / ((positiveScore + negativeScore) + 0.000001)\n",
    "    return pol_score\n",
    "\n",
    "# Calculating Average sentence length \n",
    "# It will calculated using formula --- Average Sentence Length = the number of words / the number of sentences\n",
    "     \n",
    "def average_sentence_length(text):\n",
    "    sentence_list = sent_tokenize(text)\n",
    "    tokens = tokenizer(text)\n",
    "    totalWordCount = len(tokens)\n",
    "    totalSentences = len(sentence_list)\n",
    "    average_sent = 0\n",
    "    if totalSentences != 0:\n",
    "        average_sent = totalWordCount / totalSentences\n",
    "    \n",
    "    average_sent_length= average_sent\n",
    "    \n",
    "    return round(average_sent_length)\n",
    "\n",
    "# Calculating percentage of complex word \n",
    "# It is calculated using Percentage of Complex words = the number of complex words / the number of words \n",
    "\n",
    "def percentage_complex_word(text):\n",
    "    tokens = tokenizer(text)\n",
    "    complexWord = 0\n",
    "    complex_word_percentage = 0\n",
    "    \n",
    "    for word in tokens:\n",
    "        vowels=0\n",
    "        if word.endswith(('es','ed')):\n",
    "            pass\n",
    "        else:\n",
    "            for w in word:\n",
    "                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "                    vowels += 1\n",
    "            if(vowels > 2):\n",
    "                complexWord += 1\n",
    "    if len(tokens) != 0:\n",
    "        complex_word_percentage = complexWord/len(tokens)\n",
    "    \n",
    "    return complex_word_percentage\n",
    "\n",
    "# calculating Fog Index \n",
    "# Fog index is calculated using -- Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "\n",
    "def fog_index(averageSentenceLength, percentageComplexWord):\n",
    "    fogIndex = 0.4 * (averageSentenceLength + percentageComplexWord)\n",
    "    return fogIndex\n",
    "\n",
    "# Counting complex words\n",
    "def complex_word_count(text):\n",
    "    tokens = tokenizer(text)\n",
    "    complexWord = 0\n",
    "    \n",
    "    for word in tokens:\n",
    "        vowels=0\n",
    "        if word.endswith(('es','ed')):\n",
    "            pass\n",
    "        else:\n",
    "            for w in word:\n",
    "                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "                    vowels += 1\n",
    "            if(vowels > 2):\n",
    "                complexWord += 1\n",
    "    return complexWord\n",
    "\n",
    "#Counting total words\n",
    "\n",
    "def total_word_count(text):\n",
    "    tokens = tokenizer(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# calculating uncertainty_score\n",
    "with open(uncertainty_dictionaryFile ,'r') as uncertain_dict:\n",
    "    uncertainDict=uncertain_dict.read().lower()\n",
    "uncertainDictionary = uncertainDict.split('\\n')\n",
    "\n",
    "def uncertainty_score(text):\n",
    "    uncertainWordnum =0\n",
    "    rawToken = tokenizer(text)\n",
    "    for word in rawToken:\n",
    "        if word in uncertainDictionary:\n",
    "            uncertainWordnum +=1\n",
    "    sumUncertainityScore = uncertainWordnum \n",
    "    \n",
    "    return sumUncertainityScore\n",
    "\n",
    "# calculating constraining score\n",
    "with open(constraining_dictionaryFile ,'r') as constraining_dict:\n",
    "    constrainDict=constraining_dict.read().lower()\n",
    "constrainDictionary = constrainDict.split('\\n')\n",
    "\n",
    "def constraining_score(text):\n",
    "    constrainWordnum =0\n",
    "    rawToken = tokenizer(text)\n",
    "    for word in rawToken:\n",
    "        if word in constrainDictionary:\n",
    "            constrainWordnum +=1\n",
    "    sumConstrainScore = constrainWordnum \n",
    "    \n",
    "    return sumConstrainScore\n",
    "\n",
    "# Calculating positive word proportion\n",
    "\n",
    "def positive_word_prop(positiveScore,wordcount):\n",
    "    positive_word_proportion = 0\n",
    "    if wordcount !=0:\n",
    "        positive_word_proportion = positiveScore / wordcount\n",
    "        \n",
    "    return positive_word_proportion\n",
    "\n",
    "# Calculating negative word proportion\n",
    "\n",
    "def negative_word_prop(negativeScore,wordcount):\n",
    "    negative_word_proportion = 0\n",
    "    if wordcount !=0:\n",
    "        negative_word_proportion = negativeScore / wordcount\n",
    "        \n",
    "    return negative_word_proportion\n",
    "\n",
    "# Calculating uncertain word proportion\n",
    "\n",
    "def uncertain_word_prop(uncertainScore,wordcount):\n",
    "    uncertain_word_proportion = 0\n",
    "    if wordcount !=0:\n",
    "        uncertain_word_proportion = uncertainScore / wordcount\n",
    "        \n",
    "    return uncertain_word_proportion\n",
    "\n",
    "# Calculating constraining word proportion\n",
    "\n",
    "def constraining_word_prop(constrainingScore,wordcount):\n",
    "    constraining_word_proportion = 0\n",
    "    if wordcount !=0:\n",
    "        constraining_word_proportion = constrainingScore / wordcount\n",
    "        \n",
    "    return constraining_word_proportion\n",
    "\n",
    "# calculating Constraining words for whole report\n",
    "\n",
    "def constrain_word_whole(mdaText,qqdmrText,rfText):\n",
    "    wholeDoc = mdaText + qqdmrText + rfText\n",
    "    constrainWordnumWhole =0\n",
    "    rawToken = tokenizer(wholeDoc)\n",
    "    for word in rawToken:\n",
    "        if word in constrainDictionary:\n",
    "            constrainWordnumWhole +=1\n",
    "    sumConstrainScoreWhole = constrainWordnumWhole \n",
    "    \n",
    "    return sumConstrainScoreWhole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hella\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Hella\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\Hella\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Hella\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Hella\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\Hella\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\Hella\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "C:\\Users\\Hella\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\Hella\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Hella\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Hella\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\Hella\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\Hella\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "df_text[\"positive_score\"] = df_text[\"text\"].apply(positive_score)\n",
    "df_text[\"negative_score\"] = df_text[\"text\"].apply(negative_word)\n",
    "df_text[\"polarity_score\"] = np.vectorize(polarity_score)(df_text[\"positive_score\"], df_text[\"negative_score\"])\n",
    "df_text[\"average_sen_len\"] = df_text[\"text\"].apply(average_sentence_length)\n",
    "df_text[\"percentage_complex_word\"] = df_text[\"text\"].apply(percentage_complex_word)\n",
    "df_text[\"fog_index\"] = np.vectorize(fog_index)(df_text[\"average_sen_len\"], df_text[\"percentage_complex_word\"])\n",
    "df_text[\"complex_word_count\"] = df_text[\"text\"].apply(complex_word_count)\n",
    "df_text[\"uncertainty_score\"] = df_text[\"text\"].apply(uncertainty_score)\n",
    "df_text[\"constraining_score\"] = df_text[\"text\"].apply(constraining_score)\n",
    "df_text[\"positive_word_prop\"] = np.vectorize(positive_word_prop)(df_text[\"positive_score\"], df_text[\"word_count\"])\n",
    "df_text[\"negative_word_prop\"] = np.vectorize(negative_word_prop)(df_text[\"negative_score\"],df_text[\"word_count\"])\n",
    "df_text[\"uncertain_word_prop\"] = np.vectorize(uncertain_word_prop)(df_text[\"uncertainty_score\"],df_text[\"word_count\"])\n",
    "df_text[\"constraining_word_prop\"] = np.vectorize(constraining_word_prop)(df_text[\"constraining_score\"],df_text[\"word_count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group of words creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "groups_dict = {\n",
    "     \"group_positive_all\": [\"strike\", \"greenhouse\", \"energy\", \"fuel\",\"gas\",\"emission\",\"fraudulent\",\"climate\",\"water\",\"food\",\"welfare\",\"cautionary\",\"africa\"],#positive for total\n",
    "     \"group_positive_e\": [\"weather\" , \"sustainable\", \"fleet\",\"light\",\"engine\",\"flood\",\"chemical\"], #positive for E\n",
    "     \"group_positive_s\": [\"oil\", \"consumption\", \"retiree\", \"family\",\"poor\",\"safe\",\"carbon\",\"medical\"], #positive for S\n",
    "     \"group_positive_g\": [\"cancelable\", \"insolvency\", \"differentiate\",\"metric\",\"strategically\",\"synergy\"], #positive for G\n",
    "     \"group_neative_all\": [\"noncompliance\", \"steel\", \"recoverability\",\"contaminated\",\"collusion\",\"removal\"], #negative for total\n",
    "     \"group_negative_e\": [\"unused\", \"prevented\", \"mechanical\"], #negative for E\n",
    "     \"group_negative_s\": [\"severity\", \"transmission\", \"salary\",\"threatened\"], #negative for S\n",
    "     \"group_negative_g\": [\"minor\", \"waste\",\"electricity\"] #negative for G\n",
    "     }\n",
    "\n",
    "\n",
    "groups_dat = pd.DataFrame(index=range(0,len(df_text)), columns = [\"group_positive_all\", \"group_positive_e\", \"group_positive_s\",\n",
    "                                               \"group_positive_g\", \"group_neative_all\", \"group_negative_e\",\n",
    "                                               \"group_negative_s\", \"group_negative_g\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in groups_dict.keys():\n",
    "    for i in range(len(df_text)):\n",
    "        L3 = [j for j in df_text[\"text\"][i].split() if j in groups_dict[w]]\n",
    "        groups_dat[w].iloc[i] = len(L3)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = df_text.join(groups_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a count vectorizer object \n",
    "# count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', stop_words=\"english\", max_df = 0.8, min_df = 0.2)\n",
    "# count_vect.fit(df_text['text'])\n",
    "\n",
    "# # transform the training and validation data using count vectorizer object\n",
    "# xtrain_count =  count_vect.transform(df_text[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=50, stop_words='english', max_df = 0.6, min_df=0.4)\n",
    "tfidf_vect.fit(df_text['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(df_text[\"text\"])\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=50, ngram_range=(2,2), stop_words='english', max_df = 0.6, min_df=0.4)\n",
    "tfidf_vect_ngram.fit(df_text['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(df_text[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf = pd.DataFrame(xtrain_tfidf.toarray(), columns=tfidf_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngrams = pd.DataFrame(xtrain_tfidf_ngram.toarray(), columns=tfidf_vect_ngram.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA) (Did not use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hella\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# # train a LDA Model\n",
    "# lda_model = decomposition.LatentDirichletAllocation(n_components=10, \n",
    "#                                                     learning_method='online', \n",
    "#                                                     max_iter=50,\n",
    "#                                                     random_state=123,\n",
    "#                                                     n_topics = 10,\n",
    "#                                                     evaluate_every = -1)\n",
    "# X_topics = lda_model.fit_transform(xtrain_count)\n",
    "# topic_word = lda_model.components_ \n",
    "# vocab = count_vect.get_feature_names()\n",
    "\n",
    "# # Log Likelyhood: Higher the better\n",
    "# print(\"Log Likelihood: \", lda_model.score(xtrain_count))\n",
    "\n",
    "# # Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "# print(\"Perplexity: \", lda_model.perplexity(xtrain_count))\n",
    "\n",
    "# # See model parameters\n",
    "# print(lda_model.get_params())\n",
    "\n",
    "#view the topic models\n",
    "# n_top_words = 10\n",
    "# topic_summaries = []\n",
    "# for i, topic_dist in enumerate(topic_word):\n",
    "#     topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "#     topic_summaries.append(' '.join(topic_words))\n",
    "\n",
    "# #n_topics = [10, 15, 20, 25, 30]\n",
    "# # Create Document - Topic Matrix\n",
    "# lda_output = lda_model.transform(xtrain_count)\n",
    "\n",
    "# # column names\n",
    "# topicnames = [\"Topic\" + str(i) for i in range(lda_model.n_topics)]\n",
    "\n",
    "# # index names\n",
    "# docnames = [\"Doc\" + str(i) for i in range(len(df_text))]\n",
    "\n",
    "# # Make the pandas dataframe\n",
    "# df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=range(len(df_text)))\n",
    "\n",
    "# # Get dominant topic for each document\n",
    "# dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "# df_document_topic['dominant_topic'] = dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_grams = df_tfidf.join(df_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel = df_text.join(words_grams) ## keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel.drop(columns=[\"text\", \"id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hella\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Hella\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "x = df_rel[['char_count',\n",
    "       'word_count', 'word_density', 'positive_score', 'negative_score',\n",
    "       'polarity_score', 'average_sen_len', 'percentage_complex_word',\n",
    "       'fog_index', 'complex_word_count', 'uncertainty_score',\n",
    "       'constraining_score', 'positive_word_prop', 'negative_word_prop',\n",
    "       'uncertain_word_prop', 'constraining_word_prop', 'group_positive_all',\n",
    "       'group_positive_e', 'group_positive_s', 'group_positive_g',\n",
    "       'group_neative_all', 'group_negative_e', 'group_negative_s',\n",
    "       'group_negative_g']] #returns a numpy array\n",
    "scaler = preprocessing.StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "df_rel[['char_count',\n",
    "       'word_count', 'word_density', 'positive_score', 'negative_score',\n",
    "       'polarity_score', 'average_sen_len', 'percentage_complex_word',\n",
    "       'fog_index', 'complex_word_count', 'uncertainty_score',\n",
    "       'constraining_score', 'positive_word_prop', 'negative_word_prop',\n",
    "       'uncertain_word_prop', 'constraining_word_prop', 'group_positive_all',\n",
    "       'group_positive_e', 'group_positive_s', 'group_positive_g',\n",
    "       'group_neative_all', 'group_negative_e', 'group_negative_s',\n",
    "       'group_negative_g']] = pd.DataFrame(x_scaled, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(df_rel.drop(columns=['total_esg','E_score','S_score','G_score']), \n",
    "                                                                    df_rel[['total_esg','E_score','S_score','G_score']], \n",
    "                                                                    test_size = 0.2, \n",
    "                                                                    random_state = 123)\n",
    "### Split into validation and test set\n",
    "x1_train, x1_test, y1_train, y1_test = model_selection.train_test_split(x_test, y_test, test_size = 0.5, random_state = 123)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_train1 = y_train[\"total_esg\"]\n",
    "y_train2 = y_train[\"E_score\"]\n",
    "y_train3 = y_train[\"S_score\"]\n",
    "y_train4 = y_train[\"G_score\"]\n",
    "\n",
    "y_test1 = y_test[\"total_esg\"]\n",
    "y_test2 = y_test[\"E_score\"]\n",
    "y_test3 = y_test[\"S_score\"]\n",
    "y_test4 = y_test[\"G_score\"]\n",
    "\n",
    "\n",
    "\n",
    "y1_train1 = y1_train[\"total_esg\"]\n",
    "y1_train2 = y1_train[\"E_score\"]\n",
    "y1_train3 = y1_train[\"S_score\"]\n",
    "y1_train4 = y1_train[\"G_score\"]\n",
    "\n",
    "y1_test1 = y1_test[\"total_esg\"]\n",
    "y1_test2 = y1_test[\"E_score\"]\n",
    "y1_test3 = y1_test[\"S_score\"]\n",
    "y1_test4 = y1_test[\"G_score\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_layer = [120, 100, 80]\n",
    "# second_layer = [70, 60]\n",
    "# third_layer = [50]\n",
    "# fourth_layer = [30]\n",
    "\n",
    "# # # create model\n",
    "# for first in first_layer:\n",
    "#     for second in second_layer:\n",
    "#         for third in third_layer:\n",
    "#             for fourth in fourth_layer:\n",
    "\n",
    "#                 inputs = Input(shape = (124,))\n",
    "#                 layer1 = Dense(first, activation = 'relu')(inputs)\n",
    "#                 layer1 = Dropout(0.2)(layer1)\n",
    "\n",
    "#                 layer2 = Dense(second, activation = 'relu')(layer1)\n",
    "#                 layer2 = Dropout(0.2)(layer2)\n",
    "\n",
    "\n",
    "#                 layer3 = Dense(third, activation = 'relu')(layer2)\n",
    "#                 layer3 = Dropout(0.2)(layer3)\n",
    "\n",
    "#                 layer4 = Dense(fourth, activation = 'relu')(layer3)\n",
    "#                 layer4 = Dropout(0.2)(layer4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 out1 = Dense(3, activation='softmax')(layer4)\n",
    "#                 out2 = Dense(3, activation='softmax')(layer4)\n",
    "#                 out3 = Dense(3, activation='softmax')(layer4)\n",
    "#                 out4 = Dense(3, activation='softmax')(layer4)\n",
    "#                 model = Model(inputs = inputs, outputs = [out1, out2, out3, out4])\n",
    "\n",
    "#                 # Compile the model\n",
    "#                 adamg = optimizers.adam(lr=0.001, decay=0.001)\n",
    "#                 model.compile(optimizer = 'adam',\n",
    "#                               loss = \"sparse_categorical_crossentropy\", # Call the loss function with the selected layer\n",
    "#                               metrics = ['accuracy'])\n",
    "\n",
    "#                 callbacks = [EarlyStopping(monitor='val_loss', patience=30),\n",
    "#                             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "#                 history = model.fit([x_train], [y_train1, y_train2, y_train3, y_train4],\n",
    "#                          validation_data=([x_test], [y_test1, y_test2, y_test3, y_test4]),\n",
    "#                          epochs = 800, verbose=False, callbacks=callbacks)\n",
    "\n",
    "#                 total_loss, val_loss1, val_loss2, val_loss3,val_loss4, val_acc1, val_acc2, val_acc3, val_acc4 = model.evaluate([x_test], [y_test1, y_test2, y_test3, y_test4])\n",
    "#                 print(f'accuracy:{sum([val_acc1, val_acc2, val_acc3, val_acc4])/len([val_acc1, val_acc2, val_acc3, val_acc4])*100}, first :{first}, second: {second}, third:{third}, fourth:{fourth}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Core layers\n",
    "inputs = Input(shape = (124,))\n",
    "layer1 = Dense(130, activation = 'relu')(inputs)\n",
    "layer1 = Dropout(0.2)(layer1)\n",
    "\n",
    "layer2 = Dense(80, activation = 'relu')(layer1)\n",
    "layer2 = Dropout(0.2)(layer2)\n",
    "\n",
    "\n",
    "layer3 = Dense(40, activation = 'relu', kernel_initializer='normal')(layer2)\n",
    "layer3 = Dropout(0.2)(layer3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "out1 = Dense(3, activation='softmax', kernel_initializer='normal')(layer3)\n",
    "out2 = Dense(3, activation='softmax')(layer3)\n",
    "out3 = Dense(3, activation='softmax')(layer3)\n",
    "out4 = Dense(3, activation='softmax')(layer3)\n",
    "model = Model(inputs = inputs, outputs = [out1, out2, out3, out4])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = \"sparse_categorical_crossentropy\",\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=30),\n",
    "            ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "history = model.fit([x_train], [y_train1, y_train2, y_train3, y_train4],\n",
    "         validation_data=([x1_train], [y1_train1, y1_train2, y1_train3, y1_train4]),\n",
    "         epochs = 800, verbose=False, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation loss values\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('Model loss')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377/377 [==============================] - 0s 135us/step\n",
      "51.12732087783218\n"
     ]
    }
   ],
   "source": [
    "total_loss, val_loss1, val_loss2, val_loss3,val_loss4, val_acc1, val_acc2, val_acc3, val_acc4 = model.evaluate([x1_test], [y1_test1, y1_test2, y1_test3, y1_test4])\n",
    "print(sum([val_acc1, val_acc2, val_acc3, val_acc4])/len([val_acc1, val_acc2, val_acc3, val_acc4])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy of the separate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377/377 [==============================] - 0s 116us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.313089222743593,\n",
       " 1.087343040448601,\n",
       " 1.0923970466583415,\n",
       " 1.0585465358486226,\n",
       " 1.074802675677231,\n",
       " 0.5172413791522423,\n",
       " 0.4854111398720931,\n",
       " 0.5145888581516256,\n",
       " 0.5278514579373266]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([x1_test], [y1_test1, y1_test2, y1_test3, y1_test4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.predict(x1_test)\n",
    "\n",
    "obj = pd.DataFrame(index = range(len(y1_test)))\n",
    "\n",
    "obj[\"total_esg\"] = 0\n",
    "obj[\"E_score\"] = 0\n",
    "obj[\"S_score\"] = 0\n",
    "obj[\"G_score\"] = 0\n",
    "\n",
    "for score in range(len(y1_test)):\n",
    "    obj[\"total_esg\"].iloc[score] = np.argmax(a[0][score])\n",
    "    obj[\"E_score\"].iloc[score] = np.argmax(a[1][score])\n",
    "    obj[\"S_score\"].iloc[score] = np.argmax(a[2][score])\n",
    "    obj[\"G_score\"].iloc[score] = np.argmax(a[3][score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj[\"theta\"] = 0\n",
    "def objective(y_true0, y_pred0, y_true1, y_pred1, y_true2, y_pred2, y_true3, y_pred3):\n",
    "   counter = (y_pred1 == y_true1)*1 + (y_pred2 == y_true2)*1 + (y_pred3 == y_true3)*1\n",
    "   if (counter == 0):\n",
    "       counter == 1\n",
    "   return (y_pred0 == y_true0)*counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj[\"theta\"] = np.vectorize(objective)(y1_test[\"total_esg\"],\n",
    "                                       obj[\"total_esg\"],\n",
    "                                       y1_test[\"E_score\"],\n",
    "                                       obj[\"E_score\"],\n",
    "                                       y1_test[\"S_score\"],\n",
    "                                       obj[\"S_score\"],\n",
    "                                       y1_test[\"G_score\"],\n",
    "                                       obj[\"G_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The objective function is 393\n"
     ]
    }
   ],
   "source": [
    "print(f\"The objective function is {obj.sum()[4]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
